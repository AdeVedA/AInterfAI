<p align="center">
  <img src="assets/icon.png" width="120" alt="AInterfAI Logo">
</p>

<h1 align="center"><font size="7">AInterfAI</font></h1>

<h2 align="center"><font size="6">
  <p>
    <a href="./readme.md">English</a> |
    <b>Fran√ßais</b>
  </p>
</font></h2>
<p align="center"><font size="4"><em>
Une interface graphique locale pour LLM offrant chat avanc√© avec √©dition de messages et de requ√™tes finales, configurations diverses, injection de contexte et RAG <br>- cod√© avec/pour PyQt6, LangChain, Qdrant & Ollama -
</em></font></p>

<p align="center">
  <img src="assets/session_chat.gif" width="410" alt="AInterfAI UI Screenshot">
  <img src="assets/session_coder.gif" width="410" alt="AInterfAI UI Screenshot2">
</p>

<div align="center"><font size="4">
<strong>

[‚öôÔ∏è Stack Technique](#tech-stack)
[üöÄ Fonctionnalit√©s](#features)
[‚öôÔ∏è Installation](#installation)
[‚å®Ô∏è Raccourcis Clavier](#keyboard-shortcuts)
[üóÇÔ∏è Arborescence du Projet](#file-structure)
[üìú Licence](#license)

</strong></font>

</div>

---

**AInterfAI** est une application de bureau con√ßue pour interagir avec des mod√®les de langage locaux (LLM servis localement par [Ollama](https://ollama.com)) dans un environnement productif et permettant d'enrichir les requ√™tes avec votre contexte local (documents ).

Construite avec PyQt6 et LangChain, elle supporte la gestion des sessions, la gestion de la configuration des LLM, la gestion des fichiers de contexte avec insertion compl√®te de vos documents (Full) dans vos requ√™tes ou Retrieval-Augmented Generation (RAG) sur vos propres fichiers - le RAG utilise la base de donn√©es vectorielle [Qdrant](https://qdrant.tech).

<div align="center"><font size="4">
<strong>
<h6>----------------------</h6>

<h2>(TL;DR) trop long, pas le temps de lire</h2>

[‚öôÔ∏è Installation](#installation)

USAGE :

1 - cliquez sur "+ session" (**cr√©er une session**)<br>
2 - **choisir** un **Role**, un **LLM** (r√®glages √©ventuels)<br>
3 - cliquez sur **Load LLM**<br>
(3) - si mode de contexte "Full" - choisir au moins un fichier<br>
(3) - si mode de contexte "RAG" - choisir au moins un fichier
(r√©glages √©ventuels de du nombre d'extraits K et de leur taille) et cliquez sur "Context vectorization"<br>
4 - √©crivez votre prompt. une fois termin√©, cliquez sur **ctrl+entr√©e**<br>
5 - validez votre requ√™te avec **ctrl+entr√©e**...<br>

<h6>----------------------</h6>
Pr√©sentation

</strong></font>

</div>

L'architecture s√©pare principalement deux couches :

-   **core/** : logique m√©tier, mod√®les de donn√©es, gestionnaires de configuration, gestionnaire de LLM, sous-module rag, th√®me, convertisseur tiktoken.
-   **gui/** : composants UI PyQt, workers pour le rendu et l'interaction avec le LLM, panneaux.

La partie ¬´ core ¬ª est ind√©pendante du framework UI et peut donc √™tre r√©utilis√©e pour d'autres projets LLM...  
Bien que ce pattern soit utile, il introduit des difficult√©s pour garder les composants ind√©pendants et partager un √©tat centralis√©. PyQt n√©cessite souvent de nombreux signaux dans ce type de situation.

> Aucun cloud, aucun suivi, aucune t√©l√©m√©trie : 100‚ÄØ% d'¬´‚ÄØintelligence synth√©tique‚ÄØ¬ª locale.

---

<h2 id="tech-stack">‚öôÔ∏è Stack Technique</h2>

-   **Ollama** (serveur LLM local avec API REST)
-   **PyQt6** (framework GUI)
-   **SQLAlchemy** (ORM SQLite pour le stockage persistant)
-   **Qdrant** (base de vecteurs pour le RAG)
-   **LangChain** (biblioth√®que d'int√©gration LLM)
-   **LangChain-ollama** (int√©gration Ollama pour LangChain)
-   **LangChain-qdrant** (int√©gration Qdrant pour LangChain)
-   **python-docx, python-pptx, pdfminer.six, striprtf** (modules d'extraction de texte)
-   **markdown2** (rendu Markdown)
-   **pygments** (coloration syntaxique)
-   **Configs JSON** (param√®tres g√©n√©raux de l'UI, prompts/configs de prompts, filtres du parseur de contexte)

<h2 id="features">üöÄ Fonctionnalit√©s</h2>

### üß© G√©n√©ral (Chat, Barre d'outils...)

-   Chat avec les LLM locaux via Ollama
-   **Rendu** Markdown avec coloration syntaxique
-   **Diffusion** de messages en temps r√©el
-   **Copier**, **modifier**, **supprimer** les messages
-   **Recherche** d'une cha√Æne dans le contenu des bulles de chat, avec mise en surbrillance et navigation pr√©c/suiv
-   Chargement/d√©chargement dynamique des mod√®les
-   Barre d'outils avec **indicateur d'√©tat** du LLM (vert/rouge)
-   **console** superpos√©e affichant la sortie console de l'application (cliquer sur le triangle ‚ñº en haut-√†-gauche du panneau de chat)
-   **comptage** local des **tokens** affich√© √† l'aide de `tiktoken` (session, requ√™te utilisateur, panier des fichiers de contexte)
-   Options pour :

    -   **Afficher et modifier la requ√™te finale avant envoi** au LLM (avec recherche ctrl+f aussi dedans !)
    -   **G√©n√©rer** automatiquement un **titre** de session (si le nom de la session a sa forme par d√©faut) <br> -> Je vous recommande de d√©sactiver cette option si vous avez de faibles ressources, ou si vous utilisez un gros LLM avec une longue session (√©conomie d'√©nergie et de temps).
    -   D√©finir le temps de **¬´‚ÄØkeep-alive‚ÄØ¬ª** du LLM en m√©moire
    -   D√©finir l'intervalle entre chaque sondage sur la **disponibilit√©** du LLM

### üóÇÔ∏è Gestion des Sessions

-   Multiples sessions de chat avec stockage persistant
-   **Filtrage** par date (avec vos dossiers), type de Prompt (R√¥le) ou LLM
-   Organisation par **dossiers** (et dossiers "factices" pour le filtrage des sessions par LLM ou Prompt/R√¥le)
-   Glisser-d√©poser les sessions entre dossiers (avec auto-ouverture des dossiers cibles lors du d√©p√¥t)
-   Ouvrir / fermer un dossier
-   Cr√©er automatiquement un dossier de session lorsqu'on d√©pose une session sur une autre session
-   **Renommer** (double-clic sur le nom de la session/dossier) et **supprimer** les sessions ou dossiers avec l'ic√¥ne "corbeille"
-   **Infobulle** de session (avec le dernier LLM utilis√©, type de prompt/r√¥le, date...)
-   **Exporter en markdown** une session enti√®re (tous les messages du chat) stylis√©e avec le th√®me actif, sauvegard√©e dans un fichier nomm√© {nom_de_session}.md
-   Exporter en html (wip...)

### üìö Syst√®me de Contexte

Un syst√®me modulaire pour enrichir les prompts avec vos documents (connaissances fond√©es sur vos documents injectables).

-   **Modes de Contexte**

    -   `OFF`‚ÄØ: Aucun contexte externe (requ√™te normale)
    -   `FULL CONTEXT`‚ÄØ: Injecte le contenu complet pars√© des fichiers s√©lectionn√©s
    -   `RAG`‚ÄØ: vectorise & r√©cup√®re les chunks s√©mantiquement pertinents (pour votre requ√™te) des fichiers s√©lectionn√©s via Qdrant avec le mod√®le d'embedding

    -   Formats support√©s‚ÄØ: `.pdf`, `.epub`, `.docx`, `.pptx`, `.rtf`, `.txt`, `.md`, `.xml`, `.json`, etc.

-   **Fonctionnalit√©s FULL & RAG**

    -   Param√®tres ajustables‚ÄØ: `K extracts` (nombre de chunks r√©cup√©r√©s) et `chunk size` (taille du chunk ‚âà tokens par chunk).
    -   Mod√®le d'embedding utilis√© par d√©faut‚ÄØ: `nomic-embed-text:latest` (vous pouvez changer `embedding_model` dans `core/rag/config.py` pour l'instant)
    -   Rafra√Æchir l'index (utile apr√®s mise √† jour des fichiers source)
    -   Vectorisation et indexation par fichier ou paquet de fichiers
    -   **Utilisation du RAG**‚ÄØ:
        -   s√©lectionner vos fichiers,
        -   cliquer sur **Context vectorization**,
        -   (s√©lectionner et) charger un LLM avec un r√¥le/Prompt pertinent (RAG... ou cr√©ez le v√¥tre),
        -   √©crire & envoyer votre prompt

-   **Parsing de Contexte Multi-Config**

    -   Configurations persistantes et personnalisables du parsing d'arborescence de fichiers (inclusion/exclusions/gitignore/repertoires)nomm√©es et d√©finies par l'utilisateur stock√©es dans `context_parser_config.json`.
    -   Interface d'√©dition des configurations √† onglets
    -   Contr√¥le pr√©cis des extensions de fichiers, motifs d'inclusion/exclusion avec wildcards et exclusions optionnelles `.gitignore`, nombre maximal d'enregistrements d'historique de dossiers locaux...

-   **Navigation dans l'Arborescence de Fichiers**

    -   Listing r√©cursif depuis un chemin racine (avec une limite -d√©sactivable- √† 3000 fichiers pour √©viter les risques de surmenage et inviter l'utilisateur √† resserer les filtres dans la config)
    -   Respect de `.gitignore` et des exclusions d√©finies par l'utilisateur
    -   Filtrage bas√© sur les expressions r√©guli√®res (regex)
    -   Rafra√Æchissement et scan d'un simple clic

### ‚öôÔ∏è Gestion de la Configuration LLM

-   **Configurations de prompts par d√©faut (fran√ßais ou anglais)**

    -   Les **nombreux mod√®les de r√¥le/prompts fournis** permettent de d√©finir rapidement un r√¥le/prompt syst√®me pertinent pour vos LLM.
    -   Toute **modification de la combinaison LLM + r√¥le/prompt syst√®me** et ses param√®tres associ√©s peut √™tre **sauvegard√©e**.
    -   Vous pouvez **cr√©er de nouveaux prompts** en cliquant sur ¬´ + New Role ¬ª. Si plusieurs r√¥les ou prompts syst√®me partagent le m√™me premier mot suivi d‚Äôun espace, ils seront affich√©s/regroup√©s dans un sous‚Äëmenu correspondant √† ce mot.
    -   Les r√¥les/prompts par d√©faut sont charg√©s avec un **choix de langue (fran√ßais ou anglais)** et organis√©s en dossiers selon le premier mot de leurs noms. Vous pouvez donc vous organiser comme vous le souhaitez : utilisez ¬´ + New Role ¬ª dans l‚Äôapplication ou √©ditez simplement le fichier core/prompt_config_defaults_fr.json.
    -   Lors du changement de langue (fran√ßais/anglais), le programme tente de trouver et de charger le prompt √©quivalent dans l‚Äôautre langue (par index)

-   **Propri√©t√©s du LLM**

    -   R√©cup√©ration des param√®tres par d√©faut du LLM via l'API locale d'Ollama
    -   Indication des param√®tres par d√©faut (le cas √©ch√©ant) sur les curseurs du panneau UI de configuration

-   **Configurations R√¥le/Prompt & LLM**

    -   Enregistrer/Charger‚ÄØ: ensembles Prompt/r√¥le + param√®tres LLM
    -   **Param√®tres √©ditables** (et hyper-param√®tres)‚ÄØ:
        -   prompt syst√®me
        -   temperature, top_k, repeat_penalty, top_p, min_p
        -   max tokens (avec limitation du mod√®le int√©gr√©e)
        -   flash attention (bool√©en)
        -   kv_cache_type (f16, q8_0, q4_0)
        -   use_mmap (bool√©en)
        -   num_thread (threads CPU √† utiliser)
        -   thinking (bool√©en, uniquement si le mod√®le le supporte)

### üé® Th√®mes & Apparence

-   Th√©matisation dynamique QSS avec placeholders de couleur (ex. `/*Base*/`, `/*Accent*/`)
-   Th√®mes clair/sombre via un syst√®me de palette JSON (vous pouvez personnaliser `core\theme\color_palettes.py` √† votre guise)
-   Sortie streaming Markdown avec blocs de code mis en surbrillance (autant que possible)
-   Bulles de messages avec ic√¥nes copier, √©diter et supprimer qui suivent le d√©filement (double-clic sur les bulles pour afficher/masquer ces ic√¥nes)

---

<h2 id="installation">‚öôÔ∏è Installation</h2>

### 0. Installer [Python‚ÄØ3.13+](https://www.python.org/downloads/) _des versions ant√©rieures pourraient fonctionner... je n'ai simplement pas test√© !_ et [git](https://git-scm.com/downloads)

‚Üí [https://www.python.org/downloads/](https://www.python.org/downloads/)

‚Üí [https://git-scm.com/downloads](https://git-scm.com/downloads)

### 1. R√©cup√©rer le logiciel

#### A - lancer l'interpr√©teur de commande windows

Lancez votre explorateur windows (WIN+E). Allez (dans le r√©pertoire) o√π vous souhaitez mettre le r√©pertoire d'AInterfAI. Clic gauche dans la barre d'adresse de l'explorateur, √©crivez **"cmd"** (√† la place de l'adresse) et appuyez sur **"entr√©e"** (comme √† chaque fin d'instruction en ligne future):

    cmd  # ou `terminal` sur Mac/Linux

#### B - Cr√©er un r√©pertoire pour le programme

Cr√©ez un r√©pertoire. vous pouvez l'appeler **AInterfAI** dans d:\chemin\vers\mon\dossier\AInterfAI

```bash
md AInterfAI # ou `mkdir AInterfAI` sur Mac/Linux
cd AInterfAI
```

#### C - cloner le repo Github du projet dans ce r√©pertoire

dans le terminal (l'invite de commande) qui indique bien que vous √™tes √† l'adresse du dossier cr√©√©, √©crivez :

```bash
git clone https://github.com/AdeVedA/AInterfAI
```

### 2. Cr√©er un Environnement Virtuel

```bash
python -m venv env         # ou `python3 -m venv env` sur Mac/Linux
env\Scripts\activate       # ou "source env/bin/activate" sur Mac/Linux
```

### 3. Installer les D√©pendances

```bash
pip install -r requirements.txt
```

### 4. Installer Ollama

‚Üí [https://ollama.com/download](https://ollama.com/download)

installez-le. Red√©marrez si demand√©. Une fois install√©, v√©rifiez (ou ajoutez-le) que le chemin d'installation d'Ollama est bien dans le path de vos variables d'environnement syst√®me

-   touche windows, "variables", "Modifier les variables d'environnement syst√®me", "Variables d'environnement", selectionnez la ligne "Path", cliquez sur "modifier" et v√©rifiez que le chemin vers ollama est pr√©sent.
-   Si non, cliquez sur "Nouveau"
-   ajoutez: %LOCALAPPDATA%\Programs\Ollama (ou le chemin exact dans lequel vous avez install√© Ollama)
-   Cliquez "OK" pour sauvegarder.

### 5. T√©l√©charger un LLM et un embedding

‚Üí [https://ollama.com/search](https://ollama.com/search)
Sur cette page, trouvez un mod√®le LLM dont la taille est de maximum 3/4 de votre VRAM+RAM (GB), cliquez sur son nom et copiez la commande √† ex√©cuter en terminal. Testez une fois en terminal avec une requ√™te (apr√®s avoir fait un ollama run {nom_du_model_choisi}) comme ca vous √™tes s√ªr que ca fonctionne c√¥t√© serveur ollama.

**A.** T√©l√©chargez votre premier mod√®le (voir la section [Mod√®les Ollama Recommand√©s](#modeles_recommandes) si vous √™tes perdu, ici on montre comment t√©l√©charger `mistral-small3.2:24b`)‚ÄØ:

```bash
ollama pull mistral-small3.2:24b
```

Vous pouvez utiliser n'importe quel mod√®le local compatible avec Ollama (`mistral`, `qwen3`, `gemma3`, `gpt-oss`, etc.).
Si vous avez tr√®s peu de ressources (RAM & VRAM), prenez un gemma3n:e4b, ou plus petit (mistral:7b, deepseek-r1:latest)

**B.** T√©l√©chargez le mod√®le d'embedding `"nomic-embed-text:latest"` (le RAG ne sera pas possible sans lui)‚ÄØ:

```bash
ollama pull nomic-embed-text:latest
```

### 6. Installer [Qdrant](https://github.com/qdrant/qdrant/releases)

‚Üí [https://github.com/qdrant/qdrant/releases](https://github.com/qdrant/qdrant/releases)

T√©l√©chargez le fichier correspondant √† votre os (qdrant-x86_64-pc-windows-msvc.zip pour Windows, qdrant-x86_64-apple-darwin.tar.gz pour Mac, etc..), d√©compressez/ouvrez l'archive et mettez le fichier qdrant (binary) dans un dossier de votre choix. Vous **devez** alors indiquer le chemin vers `qdrant.exe` (Windows ex.: C:\BDD\Qdrant\qdrant.exe) ou `qdrant` (mac/linux ex: C:/BDD/Qdrant/qdrant) dans le fichier `.env` √† la racine du projet (ouvrez -le avec un editeur de texte, ins√©rez le bon chemin et sauvegardez).
Sinon, autre possibilit√©, le programme vous demandera le chemin vers qdrant au premier lancement du programme et l'inscrira dans le .env automatiquement.
Vous pouvez aussi personnaliser le fichier de configuration Qdrant `config.yaml` dans le dossier `project_root\utils` si vous savez ce que vous faites.

AInterfAI pourra alors lancer Qdrant automatiquement au d√©marrage.

### 7. Lancer AInterfAI

```bash
python main.py
```

Lors du premier lancement, le programme interrogera, via des requ√™tes _locales_ √† l'API REST d'Ollama (`api/tags` & `api/show`), les informations des mod√®les afin de les enregistrer en base et de fournir des indications sur les hyper-param√®tres et propri√©t√©s recommand√©s pour chaque LLM au sein du Modelfile d'Ollama et les pr√©f√©rer √† ceux associ√©s aux r√¥les/prompts par d√©faut (qui sont agnostiques du LLM).
Si besoin, vous pouvez modifier le d√©lai (`sync_time: timedelta = timedelta(days=30)`) entre chaque parsing des propri√©t√©s LLM dans `core\llm_properties.py` (si vous mettez √† jour les Modelfile souvent).

### 8. Lancement facile (pour un d√©marrage automatis√©)

#### A. Windows

Cr√©ez un fichier nomm√© **`AInterfAI.bat`** dans le m√™me r√©pertoire que `main.py` et `env/`.  
Modifiez ce fichier avec Notepad++ ou WordPad et copiez‚Äëy le contenu suivant (enregistrez, puis double‚Äëcliquez)‚ÄØ:

```bat
@echo off
call .\env\Scripts\activate.bat
py main.py
cmd /k
```

Vous pouvez cr√©er un raccourci sur le Bureau en faisant un clic droit sur le fichier et en s√©lectionnant **‚ÄúCr√©er un raccourci‚Äù** (ou _Envoyer vers ‚Üí Bureau_).  
Une fois le raccourci cr√©√©, faites un clic droit dessus, choisissez **‚ÄúPropri√©t√©s ‚Üí Modifier l‚Äôic√¥ne‚Ä¶‚Äù**, puis parcourez `/assets/icon.ico` (ou choisissez votre propre ic√¥ne).

#### B. macOS / Linux

Cr√©ez un fichier nomm√© **`run.sh`** (ou tout autre nom de votre choix) dans le m√™me r√©pertoire que `main.py` et `env/` et copiez‚ÄØ:

```bash
source ./env/bin/activate
python3 main.py
```

Rendez le script ex√©cutable‚ÄØ:

```bash
chmod +x run.sh
```

Ex√©cutez‚Äële depuis un terminal‚ÄØ:

```bash
./run.sh
```

<h2 id="modeles_recommandes">ü§ñ Mod√®les Ollama Recommand√©s</h2>

Les r√©ponses les plus rapides proviennent de LLM enti√®rement charg√©s dans la VRAM du GPU, mais vous pouvez choisir des mod√®les plus performants (au prix d'une latence sup√©rieure) en les chargeant √† la fois en VRAM et en RAM.

**Pour le chat / usage g√©n√©ral‚ÄØ:**
| Mod√®le | VRAM / RAM min | Remarques |
| --- | --- | --- |
| `gemma3n:e4b` | pour CPU bas de gamme (>12‚ÄØGB RAM) | MOE, l√©ger et tr√®s rapide |
| `phi4:14b` | ~8‚ÄØGB VRAM + ~8‚ÄØGB RAM | Dense, l√©ger et assez rapide |
| `gpt-oss:20b` | ~6‚ÄØGB VRAM + ~12‚ÄØGB RAM | MOE, rapide, performant |
| `qwen3:30b-a3b` | ~8‚ÄØGB VRAM + ~16‚ÄØGB RAM | MOE, rapide, performant |
| `qwen3:30b` | ~8‚ÄØGB VRAM + ~16‚ÄØGB RAM | Dense, rapide, performant |
| `gemma3:27b-it-qat` | ~12‚ÄØGB VRAM + ~16‚ÄØGB RAM | Dense, bon compromis, quantification optimis√©e (qat) |
| `mistral-small3.2:24b` | ~8‚ÄØGB VRAM + ~16‚ÄØGB RAM | Dense, bon compromis, performant |
| `gpt-oss:120b` | ~16‚ÄØGB VRAM + ~64‚ÄØGB RAM | MOE, tr√®s grand, plus pr√©cis |

**Pour le codage‚ÄØ:**
| Mod√®le | VRAM / RAM min | Remarques |
| --- | --- | --- |
| `gpt-oss:20b` | ~6‚ÄØGB VRAM + ~12‚ÄØGB RAM | MOE, rapide, performant |
| `qwen3-coder:30b-a3b` | ~8‚ÄØGB VRAM + ~16‚ÄØGB RAM | MOE, rapide, performant |
| `gemma3:27b-it-qat` | ~12‚ÄØGB VRAM + ~16‚ÄØGB RAM | Dense, bon compromis, quantification optimis√©e (qat) |
| `qwen3-coder:30b` | ~8‚ÄØGB VRAM + ~16‚ÄØGB RAM | Dense, encore plus performant |
| `magistral:24b` | ~8‚ÄØGB VRAM + ~16‚ÄØGB RAM | Dense, tr√®s performant |
| `gpt-oss:120b` | ~16‚ÄØGB VRAM + ~64‚ÄØGB RAM | MOE, tr√®s grand, plus pr√©cis |

_Note pour les d√©butants‚ÄØ:_ les LLM MOE (Mixture-Of-Experts) sont plus rapides et moins gourmands en ressources que les LLM denses.

---

<h2 id="keyboard-shortcuts">‚å®Ô∏è Raccourcis Clavier</h2>

| Raccourci                  | Contexte                                                                                                                                                                                                                   |
| -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Ctrl + Molette de souris` | Zoom in/out de la police (dans les bulles de chat)                                                                                                                                                                         |
| `PageUp/PageDown`          | Navigation pagin√©e (dans les bulles de chat et autres zones de texte)                                                                                                                                                      |
| `CTRL + F`                 | Recherche du `mot` s√©lectionn√© dans le chat (et dans le dialogue de validation pr√©-inference),<br>surbrillance des r√©sultats avec pr√©c√©dent/suivant & surbrillance des r√©sultats (dans les messages d'une session ouverte) |
| `CTRL + Enter`             | Envoyer le message (lorsque le champ de saisie du chat a le focus)                                                                                                                                                         |
| `Escape`                   | Annuler (en cours d'√©dition d'un message, lors d'une recherche)                                                                                                                                                            |
| `CTRL + S`                 | Confirmer & sauvegarder (en cours d'√©dition d'un message)                                                                                                                                                                  |
| `Enter`                    | Parcourir l'arborescence de fichiers <br>(lorsque la bo√Æte de chemin `Root folder` a le focus)                                                                                                                             |
| `‚Üë / ‚Üì`                    | Naviguer parmi les chemins r√©cents (dans la bo√Æte de chemin `Root folder`)                                                                                                                                                 |

---

<h2 id="file-structure">üóÇÔ∏è Arborescence du Projet</h2>

```
project_root/
‚îú‚îÄ‚îÄ main.py                         # Point d'entr√©e de l'application
‚îú‚îÄ‚îÄ requirements.txt                # D√©pendances Python
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ config.yaml                     # Configuration Qdrant
‚îÇ
‚îú‚îÄ‚îÄ core/                               # Logique back-end (LangChain, DB, parsing,...)
‚îÇ   ‚îú‚îÄ‚îÄ config_manager.py           # Gestionnaire de configurations R√¥le/Prompt
‚îÇ   ‚îú‚îÄ‚îÄ context_parser_config.json  # Configs multi-mode pour la gestion du contexte
‚îÇ   ‚îú‚îÄ‚îÄ context_parser.py           # Logique de gestion du contexte
‚îÇ   ‚îú‚îÄ‚îÄ database.py                 # Session DB SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ llm_manager.py              # Gestionnaire de LLM
‚îÇ   ‚îú‚îÄ‚îÄ llm_properties.py           # Gestionnaire des propri√©t√©s par d√©faut du LLM
‚îÇ   ‚îú‚îÄ‚îÄ models.py                   # Mod√®les ORM (Session, Message, Folder, llm_properties, prompt_config)
‚îÇ   ‚îú‚îÄ‚îÄ prompt_config_defaults.json # Configs par d√©faut R√¥le/Prompt
‚îÇ   ‚îú‚îÄ‚îÄ prompt_config_manager.py    # Gestionnaire de configurations R√¥le/Prompt
‚îÇ   ‚îú‚îÄ‚îÄ prompt_manager.py           # Construit les prompts LLM √† partir des configs
‚îÇ   ‚îú‚îÄ‚îÄ session_manager.py          # Gestion des sessions (et leurs messages)
‚îÇ   ‚îú‚îÄ‚îÄ message_manager/                  # Module de gestion des messages
‚îÇ       ‚îú‚îÄ‚îÄ msg_proc.py             # Gestionnaire de traitement des messages
‚îÇ       ‚îú‚îÄ‚îÄ msg_proc_utils.py       # Utilitaires de traitement des messages
‚îÇ   ‚îú‚îÄ‚îÄ rag/                              # Module de gestion du rag
‚îÇ       ‚îú‚îÄ‚îÄ handler.py              # Gestionnaire global du rag
‚îÇ       ‚îú‚îÄ‚îÄ file_loader.py          # Extracteurs de texte pour les formats support√©s
‚îÇ       ‚îú‚îÄ‚îÄ indexer.py              # Indexation et r√©cup√©ration de chunks Qdrant
‚îÇ       ‚îú‚îÄ‚îÄ config.py               # Configuration du pipeline rag
‚îÇ   ‚îú‚îÄ‚îÄ theme/                            # Module de th√©matisation
‚îÇ       ‚îú‚îÄ‚îÄ theme_manager.py        # Gestionnaire de th√©matisation QSS dynamique
‚îÇ       ‚îú‚îÄ‚îÄ color_palettes.py       # Palettes de couleurs pour injection QSS
‚îÇ       ‚îú‚îÄ‚îÄ theme.qss               # Feuille de style QSS
‚îÇ   ‚îú‚îÄ‚îÄ tiktoken/                         # Module tiktoken
‚îÇ       ‚îú‚îÄ‚îÄ 9b5ad...                # mod√®le tiktoken local pour comptage de tokens
‚îÇ
‚îú‚îÄ‚îÄ gui/                                # Composants GUI PyQt6
‚îÇ   ‚îú‚îÄ‚îÄ chat_panel.py               # Interface de chat LLM avec streaming
‚îÇ   ‚îú‚îÄ‚îÄ config_panel.py             # Panneau de gestion des param√®tres LLM
‚îÇ   ‚îú‚îÄ‚îÄ context_parser_panel.py     # Panneau pour s√©lection de mode de contexte & s√©lection de fichiers
‚îÇ   ‚îú‚îÄ‚îÄ gui_config.json             # Param√®tres persistants de l'UI
‚îÇ   ‚îú‚îÄ‚îÄ gui.py                      # Fen√™tre principale MainWindow
‚îÇ   ‚îú‚îÄ‚îÄ llm_worker.py               # Thread pour le streaming LLM
‚îÇ   ‚îú‚îÄ‚îÄ render_worker.py            # Thread du parseur Markdown
‚îÇ   ‚îú‚îÄ‚îÄ renderer.py                 # Parseur Markdown avec coloration syntaxique
‚îÇ   ‚îú‚îÄ‚îÄ session_panel.py            # Arbre dossier/session avec drag & drop, vue et gestion des sessions
‚îÇ   ‚îú‚îÄ‚îÄ thread_manager.py           # Gestionnaire de threads (QThread et threading.Thread...)
‚îÇ   ‚îú‚îÄ‚îÄ toolbar.py                  # Panneau de barre d'outils
‚îÇ   ‚îú‚îÄ‚îÄ widgets/                          # Petit module de widgets
‚îÇ       ‚îú‚îÄ‚îÄ context_config_dialog.py# Dialogue de configuration de contexte
‚îÇ       ‚îú‚îÄ‚îÄ prompt_validation_dialog.py # Dialogue de validation de votre requ√™te, editable avant envoi pour inference
‚îÇ       ‚îú‚îÄ‚îÄ small_widget.py         # Petits widgets
‚îÇ       ‚îú‚îÄ‚îÄ spinner.py              # Spinner ¬´‚ÄØprocessing...‚ÄØ¬ª
‚îÇ       ‚îú‚îÄ‚îÄ status_indicator.py     # Indicateur vert/rouge LLM charg√©/d√©charg√©
‚îÇ       ‚îú‚îÄ‚îÄ search_dialog.py        # Bo√Æte de recherche CTRL+F, surbrillance des r√©sultats dans les bulles avec navigation prev/next
‚îÇ
‚îú‚îÄ‚îÄ utils/                              # Modules utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ qdrant_launcher.py          # Gestion du lancement/arr√™t du binaire Qdrant
‚îÇ   ‚îú‚îÄ‚îÄ env_tools.py                # Entr√©e CLI utilisateur pour placer le chemin Qdrant.exe dans .env si besoin
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml                 # Config Qdrant
‚îÇ
```

---

## üîÆ Perspectives Futures

-   trouver un emploi‚ÄØ! <--- IMPORTANT apr√®s presque 5 mois sur cette application !!!!
-   Panneau de chat‚ÄØ: ¬´‚ÄØcontinue‚ÄØ/‚ÄØregenerate‚ÄØ¬ª des r√©ponses LLM
-   Sauvegarder les messages rendus en HTML pour √©viter le rendu √† la vol√©e
-   Abstraction de la gestion des serveurs LLM afin d'int√©grer llamacpp et/ou LMStudio comme fournisseur LLM
-   Autoriser l'usage de l'API OpenAI pour des requ√™tes LLM distantes
-   Possibilit√© d'imbriquer des dossiers de session dans d'autres dossiers de session
-   Cr√©ation et orchestration d'agents LangChain
-   R√©sum√© structur√© multi-fichiers bas√© sur le RAG
-   Migration de la base SQLite vers PostgreSQL(...?)
-   Int√©gration de la gestion d'images pour les LLM capables de vision
-   Gestion multilingue de l'interface‚ÄØ: traductions... (pour les r√¥les/prompt par d√©faut, c'est d√©j√† fait!)
-   Collaborations‚ÄØ?

---

<h2 id="license">üìú Licence</h2>

Ce projet est distribu√© sous licence GPL‚ÄØv3. Voir le fichier [LICENSE](https://github.com/python-qt-tools/PyQt6-stubs/blob/main/LICENSE) pour plus de d√©tails.

### Licences Tierces

-   [PyQt6](https://github.com/python-qt-tools/PyQt6-stubs/blob/main/LICENSE) - GPL‚ÄØv3
-   [LangChain](https://github.com/langchain-ai/langchain/blob/master/LICENSE) - MIT
-   [Qdrant](https://github.com/qdrant/qdrant/blob/master/LICENSE) - Apache‚ÄØ2.0
-   [Ollama](https://github.com/ollama/ollama/blob/main/LICENSE) - MIT
