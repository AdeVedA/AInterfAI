<p align="center">
  <img src="assets/icon.png" width="120" alt="AInterfAI Logo">
</p>

<h1 align="center"><font size="7">AInterfAI</font></h1>

<h2 align="center"><font size="6">
  <p>
    <a href="./readme.md">English</a> |
    <b>FranÃ§ais</b>
  </p>
</font></h2>
<p align="center"><font size="4"><em>
Une interface graphique locale pour LLM offrant chat avancÃ© avec Ã©dition de messages et de requÃªtes finales, configurations diverses, injection de contexte et RAG <br>- codÃ© avec/pour PyQt6, LangChain, Qdrant & Ollama -
</em></font></p>

<p align="center">
  <img src="assets/session_chat.gif" width="410" alt="AInterfAI UI Screenshot">
  <img src="assets/session_coder.gif" width="410" alt="AInterfAI UI Screenshot2">
</p>

<div align="center"><font size="4">
<strong>

[âš™ï¸ Stack Technique](#tech-stack)
[ğŸš€ FonctionnalitÃ©s](#features)
[âš™ï¸ Installation](#installation)
[âŒ¨ï¸ Raccourcis Clavier](#keyboard-shortcuts)
[ğŸ—‚ï¸ Arborescence du Projet](#file-structure)
[ğŸ“œ Licence](#license)

</strong></font>

</div>

---

**AInterfAI** est une application de bureau conÃ§ue pour interagir avec des modÃ¨les de langage locaux (LLM servis localement par [Ollama](https://ollama.com)) dans un environnement productif et permettant d'enrichir les requÃªtes avec votre contexte local (documents ).

Construite avec PyQt6 et LangChain, elle supporte la gestion des sessions, la gestion de la configuration des LLM, la gestion des fichiers de contexte avec insertion complÃ¨te de vos documents (Full) dans vos requÃªtes ou Retrieval-Augmented Generation (RAG) sur vos propres fichiers - le RAG utilise la base de donnÃ©es vectorielle [Qdrant](https://qdrant.tech).

<div align="center"><font size="4">
<strong>
<h6>----------------------</h6>

<h2>(TL;DR) trop long, pas le temps de lire</h2>

[âš™ï¸ Installation](#installation)

USAGE :

1 - cliquez sur "+ session" (**crÃ©er une session**)<br>
2 - **choisir** un **Role**, un **LLM** (rÃ¨glages Ã©ventuels)<br>
3 - cliquez sur **Load LLM**<br>
(3) - si mode de contexte "Full" - choisir au moins un fichier<br>
(3) - si mode de contexte "RAG" - choisir au moins un fichier
(rÃ©glages Ã©ventuels de du nombre d'extraits K et de leur taille) et cliquez sur "Context vectorization"<br>
4 - Ã©crivez votre prompt. une fois terminÃ©, cliquez sur **ctrl+entrÃ©e**<br>
5 - validez votre requÃªte avec **ctrl+entrÃ©e**...<br>

<h6>----------------------</h6>
PrÃ©sentation

</strong></font>

</div>

L'architecture sÃ©pare principalement deux couches :

-   **core/** : logique mÃ©tier, modÃ¨les de donnÃ©es, gestionnaires de configuration, gestionnaire de LLM, sous-module rag, thÃ¨me, convertisseur tiktoken.
-   **gui/** : composants UI PyQt, workers pour le rendu et l'interaction avec le LLM, panneaux.

La partie Â« core Â» est indÃ©pendante du framework UI et peut donc Ãªtre rÃ©utilisÃ©e pour d'autres projets LLM...  
Bien que ce pattern soit utile, il introduit des difficultÃ©s pour garder les composants indÃ©pendants et partager un Ã©tat centralisÃ©. PyQt nÃ©cessite souvent de nombreux signaux dans ce type de situation.

> Aucun cloud, aucun suivi, aucune tÃ©lÃ©mÃ©trie : 100â€¯% d'Â«â€¯intelligence synthÃ©tiqueâ€¯Â» locale.

---

<h2 id="tech-stack">âš™ï¸ Stack Technique</h2>

-   **Ollama** (serveur LLM local avec API REST)
-   **PyQt6** (framework GUI)
-   **SQLAlchemy** (ORM SQLite pour le stockage persistant)
-   **Qdrant** (base de vecteurs pour le RAG)
-   **LangChain** (bibliothÃ¨que d'intÃ©gration LLM)
-   **LangChain-ollama** (intÃ©gration Ollama pour LangChain)
-   **LangChain-qdrant** (intÃ©gration Qdrant pour LangChain)
-   **python-docx, python-pptx, pdfminer.six, striprtf** (modules d'extraction de texte)
-   **markdown2** (rendu Markdown)
-   **pygments** (coloration syntaxique)
-   **Configs JSON** (paramÃ¨tres gÃ©nÃ©raux de l'UI, prompts/configs de prompts, filtres du parseur de contexte)

<h2 id="features">ğŸš€ FonctionnalitÃ©s</h2>

### ğŸ§© GÃ©nÃ©ral (Chat, Barre d'outils...)

-   Chat avec les LLM locaux via Ollama
-   **Rendu** Markdown avec coloration syntaxique
-   **Diffusion** de messages en temps rÃ©el
-   **Copier**, **modifier**, **supprimer** les messages
-   **Recherche** d'une chaÃ®ne dans le contenu des bulles de chat, avec mise en surbrillance et navigation prÃ©c/suiv
-   Chargement/dÃ©chargement dynamique des modÃ¨les
-   Barre d'outils avec **indicateur d'Ã©tat** du LLM (vert/rouge)
-   **console** superposÃ©e affichant la sortie console de l'application (cliquer sur le triangle â–¼ en haut-Ã -gauche du panneau de chat)
-   **comptage** local des **tokens** affichÃ© Ã  l'aide de `tiktoken` (session, requÃªte utilisateur, panier des fichiers de contexte)
-   Options pour :

    -   **Afficher et modifier la requÃªte finale avant envoi** au LLM (avec recherche ctrl+f aussi dedans !)
    -   **GÃ©nÃ©rer** automatiquement un **titre** de session (si le nom de la session a sa forme par dÃ©faut) <br> -> Je vous recommande de dÃ©sactiver cette option si vous avez de faibles ressources, ou si vous utilisez un gros LLM avec une longue session (Ã©conomie d'Ã©nergie et de temps).
    -   DÃ©finir le temps de **Â«â€¯keep-aliveâ€¯Â»** du LLM en mÃ©moire
    -   DÃ©finir l'intervalle entre chaque sondage sur la **disponibilitÃ©** du LLM

### ğŸ—‚ï¸ Gestion des Sessions

-   Multiples sessions de chat avec stockage persistant
-   **Filtrage** par date (avec vos dossiers), type de Prompt (RÃ´le) ou LLM
-   Organisation par **dossiers** (et dossiers "factices" pour le filtrage des sessions par LLM ou Prompt/RÃ´le)
-   Glisser-dÃ©poser les sessions entre dossiers (avec auto-ouverture des dossiers cibles lors du dÃ©pÃ´t)
-   Ouvrir / fermer un dossier
-   CrÃ©er automatiquement un dossier de session lorsqu'on dÃ©pose une session sur une autre session
-   **Renommer** (double-clic sur le nom de la session/dossier) et **supprimer** les sessions ou dossiers avec l'icÃ´ne "corbeille"
-   **Infobulle** de session (avec le dernier LLM utilisÃ©, type de prompt/rÃ´le, date...)
-   **Exporter en markdown** une session entiÃ¨re (tous les messages du chat) stylisÃ©e avec le thÃ¨me actif, sauvegardÃ©e dans un fichier nommÃ© {nom_de_session}.md
-   Exporter en html (wip...)

### ğŸ“š SystÃ¨me de Contexte

Un systÃ¨me modulaire pour enrichir les prompts avec vos documents (connaissances fondÃ©es sur vos documents injectables).

-   **Modes de Contexte**

    -   `OFF`â€¯: Aucun contexte externe (requÃªte normale)
    -   `FULL CONTEXT`â€¯: Injecte le contenu complet parsÃ© des fichiers sÃ©lectionnÃ©s
    -   `RAG`â€¯: vectorise & rÃ©cupÃ¨re les chunks sÃ©mantiquement pertinents (pour votre requÃªte) des fichiers sÃ©lectionnÃ©s via Qdrant avec le modÃ¨le d'embedding

    -   Formats supportÃ©sâ€¯: `.pdf`, `.epub`, `.docx`, `.pptx`, `.rtf`, `.txt`, `.md`, `.xml`, `.json`, etc.

-   **FonctionnalitÃ©s FULL & RAG**

    -   ParamÃ¨tres ajustablesâ€¯: `K extracts` (nombre de chunks rÃ©cupÃ©rÃ©s) et `chunk size` (taille du chunk â‰ˆ tokens par chunk).
    -   ModÃ¨le d'embedding utilisÃ© par dÃ©fautâ€¯: `nomic-embed-text:latest` (vous pouvez changer `embedding_model` dans `core/rag/config.py` pour l'instant)
    -   RafraÃ®chir l'index (utile aprÃ¨s mise Ã  jour des fichiers source)
    -   Vectorisation et indexation par fichier ou paquet de fichiers
    -   **Utilisation du RAG**â€¯:
        -   sÃ©lectionner vos fichiers,
        -   cliquer sur **Context vectorization**,
        -   (sÃ©lectionner et) charger un LLM avec un rÃ´le/Prompt pertinent (RAG... ou crÃ©ez le vÃ´tre),
        -   Ã©crire & envoyer votre prompt

-   **Parsing de Contexte Multi-Config**

    -   Configurations persistantes et personnalisables du parsing d'arborescence de fichiers (inclusion/exclusions/gitignore/repertoires)nommÃ©es et dÃ©finies par l'utilisateur stockÃ©es dans `context_parser_config.json`.
    -   Interface d'Ã©dition des configurations Ã  onglets
    -   ContrÃ´le prÃ©cis des extensions de fichiers, motifs d'inclusion/exclusion avec wildcards et exclusions optionnelles `.gitignore`, nombre maximal d'enregistrements d'historique de dossiers locaux...

-   **Navigation dans l'Arborescence de Fichiers**

    -   Listing rÃ©cursif depuis un chemin racine (avec une limite -dÃ©sactivable- Ã  3000 fichiers pour Ã©viter les risques de surmenage et inviter l'utilisateur Ã  resserer les filtres dans la config)
    -   Respect de `.gitignore` et des exclusions dÃ©finies par l'utilisateur
    -   Filtrage basÃ© sur les expressions rÃ©guliÃ¨res (regex)
    -   RafraÃ®chissement et scan d'un simple clic

### âš™ï¸ Gestion de la Configuration LLM

-   **Configurations de prompts par dÃ©faut (franÃ§ais ou anglais)**

    -   Les **nombreux modÃ¨les de rÃ´le/prompts fournis** permettent de dÃ©finir rapidement un rÃ´le/prompt systÃ¨me pertinent pour vos LLM.
    -   Toute **modification de la combinaison LLM + rÃ´le/prompt systÃ¨me** et ses paramÃ¨tres associÃ©s peut Ãªtre **sauvegardÃ©e**.
    -   Vous pouvez **crÃ©er de nouveaux prompts** en cliquant sur Â« + New Role Â». Si plusieurs rÃ´les ou prompts systÃ¨me partagent le mÃªme premier mot suivi dâ€™un espace, ils seront affichÃ©s/regroupÃ©s dans un sousâ€‘menu correspondant Ã  ce mot.
    -   Les rÃ´les/prompts par dÃ©faut sont chargÃ©s avec un **choix de langue (franÃ§ais ou anglais)** et organisÃ©s en dossiers selon le premier mot de leurs noms. Vous pouvez donc vous organiser comme vous le souhaitez : utilisez Â« + New Role Â» dans lâ€™application ou Ã©ditez simplement le fichier core/prompt_config_defaults_fr.json.
    -   Lors du changement de langue (franÃ§ais/anglais), le programme tente de trouver et de charger le prompt Ã©quivalent dans lâ€™autre langue (par index)

-   **PropriÃ©tÃ©s du LLM**

    -   RÃ©cupÃ©ration des paramÃ¨tres par dÃ©faut du LLM via l'API locale d'Ollama
    -   Indication des paramÃ¨tres par dÃ©faut (le cas Ã©chÃ©ant) sur les curseurs du panneau UI de configuration

-   **Configurations RÃ´le/Prompt & LLM**

    -   Enregistrer/Chargerâ€¯: ensembles Prompt/rÃ´le + paramÃ¨tres LLM
    -   **ParamÃ¨tres Ã©ditables** (et hyper-paramÃ¨tres)â€¯:
        -   prompt systÃ¨me
        -   temperature, top_k, repeat_penalty, top_p, min_p
        -   max tokens (avec limitation du modÃ¨le intÃ©grÃ©e)
        -   flash attention (boolÃ©en)
        -   kv_cache_type (f16, q8_0, q4_0)
        -   use_mmap (boolÃ©en)
        -   num_thread (threads CPU Ã  utiliser)
        -   thinking (boolÃ©en, uniquement si le modÃ¨le le supporte)

### ğŸ¨ ThÃ¨mes & Apparence

-   ThÃ©matisation dynamique QSS avec placeholders de couleur (ex. `/*Base*/`, `/*Accent*/`)
-   ThÃ¨mes clair/sombre via un systÃ¨me de palette JSON (vous pouvez personnaliser `core\theme\color_palettes.py` Ã  votre guise)
-   Sortie streaming Markdown avec blocs de code mis en surbrillance (autant que possible)
-   Bulles de messages avec icÃ´nes copier, Ã©diter et supprimer qui suivent le dÃ©filement (double-clic sur les bulles pour afficher/masquer ces icÃ´nes)

---

<h2 id="installation">âš™ï¸ Installation</h2>

### 0. Installer [Pythonâ€¯3.13+](https://www.python.org/downloads/) _des versions antÃ©rieures pourraient fonctionner... je n'ai simplement pas testÃ© !_ et [git](https://git-scm.com/downloads)

â†’ [https://www.python.org/downloads/](https://www.python.org/downloads/)

â†’ [https://git-scm.com/downloads](https://git-scm.com/downloads)

### 1. RÃ©cupÃ©rer le logiciel

#### A - lancer l'interprÃ©teur de commande windows

Lancez votre explorateur windows (WIN+E). Allez (dans le rÃ©pertoire) oÃ¹ vous souhaitez mettre le rÃ©pertoire d'AInterfAI. Clic gauche dans la barre d'adresse de l'explorateur, Ã©crivez **"cmd"** (Ã  la place de l'adresse) et appuyez sur **"entrÃ©e"** (comme Ã  chaque fin d'instruction en ligne future):

    cmd  # ou `terminal` sur Mac/Linux

#### B - CrÃ©er un rÃ©pertoire pour le programme

CrÃ©ez un rÃ©pertoire. vous pouvez l'appeler **AInterfAI** dans d:\chemin\vers\mon\dossier\AInterfAI

```bash
md AInterfAI # ou `mkdir AInterfAI` sur Mac/Linux
cd AInterfAI
```

#### C - cloner le repo Github du projet dans ce rÃ©pertoire

dans le terminal (l'invite de commande) qui indique bien que vous Ãªtes Ã  l'adresse du dossier crÃ©Ã©, Ã©crivez :

```bash
git clone https://github.com/AdeVedA/AInterfAI
```

### 2. CrÃ©er un Environnement Virtuel

```bash
python -m venv env         # ou `python3 -m venv env` sur Mac/Linux
env\Scripts\activate       # ou "source env/bin/activate" sur Mac/Linux
```

### 3. Installer les DÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. Installer Ollama

â†’ [https://ollama.com/download](https://ollama.com/download)

installez-le. RedÃ©marrez si demandÃ©. Une fois installÃ©, vÃ©rifiez (ou ajoutez-le) que le chemin d'installation d'Ollama est bien dans le path de vos variables d'environnement systÃ¨me

-   touche windows, "variables", "Modifier les variables d'environnement systÃ¨me", "Variables d'environnement", selectionnez la ligne "Path", cliquez sur "modifier" et vÃ©rifiez que le chemin vers ollama est prÃ©sent.
-   Si non, cliquez sur "Nouveau"
-   ajoutez: %LOCALAPPDATA%\Programs\Ollama (ou le chemin exact dans lequel vous avez installÃ© Ollama)
-   Cliquez "OK" pour sauvegarder.

### 5. TÃ©lÃ©charger un LLM et un embedding

â†’ [https://ollama.com/search](https://ollama.com/search)
Sur cette page, trouvez un modÃ¨le LLM dont la taille est de maximum 3/4 de votre VRAM+RAM (GB), cliquez sur son nom et copiez la commande Ã  exÃ©cuter en terminal. Testez une fois en terminal avec une requÃªte (aprÃ¨s avoir fait un ollama run {nom_du_model_choisi}) comme ca vous Ãªtes sÃ»r que ca fonctionne cÃ´tÃ© serveur ollama.

**A.** TÃ©lÃ©chargez votre premier modÃ¨le (voir la section [ModÃ¨les Ollama RecommandÃ©s](#modeles_recommandes) si vous Ãªtes perdu, ici on montre comment tÃ©lÃ©charger `mistral-small3.2:24b`)â€¯:

```bash
ollama pull mistral-small3.2:24b
```

Vous pouvez utiliser n'importe quel modÃ¨le local compatible avec Ollama (`mistral`, `qwen3`, `gemma3`, `gpt-oss`, etc.).
Si vous avez trÃ¨s peu de ressources (RAM & VRAM), prenez un gemma3n:e4b, ou plus petit (mistral:7b, deepseek-r1:latest)

**B.** TÃ©lÃ©chargez le modÃ¨le d'embedding `"nomic-embed-text:latest"` (le RAG ne sera pas possible sans lui)â€¯:

```bash
ollama pull nomic-embed-text:latest
```

### 6. Installer [Qdrant](https://github.com/qdrant/qdrant/releases)

â†’ [https://github.com/qdrant/qdrant/releases](https://github.com/qdrant/qdrant/releases)

TÃ©lÃ©chargez le fichier correspondant Ã  votre os (qdrant-x86_64-pc-windows-msvc.zip pour Windows, qdrant-x86_64-apple-darwin.tar.gz pour Mac, etc..), dÃ©compressez/ouvrez l'archive et mettez le fichier qdrant (binary) dans un dossier de votre choix. Vous **devez** alors indiquer le chemin vers `qdrant.exe` (Windows ex.: C:\BDD\Qdrant\qdrant.exe) ou `qdrant` (mac/linux ex: C:/BDD/Qdrant/qdrant) dans le fichier `.env` Ã  la racine du projet (ouvrez -le avec un editeur de texte, insÃ©rez le bon chemin et sauvegardez).
Sinon, autre possibilitÃ©, le programme vous demandera le chemin vers qdrant au premier lancement du programme et l'inscrira dans le .env automatiquement.
Vous pouvez aussi personnaliser le fichier de configuration Qdrant `config.yaml` dans le dossier `project_root\utils` si vous savez ce que vous faites.

AInterfAI pourra alors lancer Qdrant automatiquement au dÃ©marrage.

### 7. Lancer AInterfAI

```bash
python main.py
```

Lors du premier lancement, le programme interrogera, via des requÃªtes _locales_ Ã  l'API REST d'Ollama (`api/tags` & `api/show`), les informations des modÃ¨les afin de les enregistrer en base et de fournir des indications sur les hyper-paramÃ¨tres et propriÃ©tÃ©s recommandÃ©s pour chaque LLM au sein du Modelfile d'Ollama et les prÃ©fÃ©rer Ã  ceux associÃ©s aux rÃ´les/prompts par dÃ©faut (qui sont agnostiques du LLM).
Si besoin, vous pouvez modifier le dÃ©lai (`sync_time: timedelta = timedelta(days=30)`) entre chaque parsing des propriÃ©tÃ©s LLM dans `core\llm_properties.py` (si vous mettez Ã  jour les Modelfile souvent).

### 8. Lancement facile (pour un dÃ©marrage automatisÃ©)

#### A. Windows

CrÃ©ez un fichier nommÃ© **`AInterfAI.bat`** dans le mÃªme rÃ©pertoire que `main.py` et `env/`.  
Modifiez ce fichier avec Notepad++ ou WordPad et copiezâ€‘y le contenu suivant (enregistrez, puis doubleâ€‘cliquez)â€¯:

```bat
@echo off
call .\env\Scripts\activate.bat
py main.py
cmd /k
```

Vous pouvez crÃ©er un raccourci sur le Bureau en faisant un clic droit sur le fichier et en sÃ©lectionnant **â€œCrÃ©er un raccourciâ€** (ou _Envoyer vers â†’ Bureau_).  
Une fois le raccourci crÃ©Ã©, faites un clic droit dessus, choisissez **â€œPropriÃ©tÃ©s â†’ Modifier lâ€™icÃ´neâ€¦â€**, puis parcourez `/assets/icon.ico` (ou choisissez votre propre icÃ´ne).

#### B. macOS / Linux

CrÃ©ez un fichier nommÃ© **`run.sh`** (ou tout autre nom de votre choix) dans le mÃªme rÃ©pertoire que `main.py` et `env/` et copiezâ€¯:

```bash
source ./env/bin/activate
python3 main.py
```

Rendez le script exÃ©cutableâ€¯:

```bash
chmod +x run.sh
```

ExÃ©cutezâ€‘le depuis un terminalâ€¯:

```bash
./run.sh
```

<h2 id="modeles_recommandes">ğŸ¤– ModÃ¨les Ollama RecommandÃ©s</h2>

Les rÃ©ponses les plus rapides proviennent de LLM entiÃ¨rement chargÃ©s dans la VRAM du GPU, mais vous pouvez choisir des modÃ¨les plus performants (au prix d'une latence supÃ©rieure) en les chargeant Ã  la fois en VRAM et en RAM.

**Pour le chat / usage gÃ©nÃ©ralâ€¯:**
| ModÃ¨le | VRAM / RAM min | Remarques |
| --- | --- | --- |
| `gemma3n:e4b` | pour CPU bas de gamme (>12â€¯GB RAM) | MOE, lÃ©ger et trÃ¨s rapide |
| `phi4:14b` | ~8â€¯GB VRAM + ~8â€¯GB RAM | Dense, lÃ©ger et assez rapide |
| `gpt-oss:20b` | ~6â€¯GB VRAM + ~12â€¯GB RAM | MOE, rapide, performant |
| `qwen3:30b-a3b` | ~8â€¯GB VRAM + ~16â€¯GB RAM | MOE, rapide, performant |
| `qwen3:30b` | ~8â€¯GB VRAM + ~16â€¯GB RAM | Dense, rapide, performant |
| `gemma3:27b-it-qat` | ~12â€¯GB VRAM + ~16â€¯GB RAM | Dense, bon compromis, quantification optimisÃ©e (qat) |
| `mistral-small3.2:24b` | ~8â€¯GB VRAM + ~16â€¯GB RAM | Dense, bon compromis, performant |
| `gpt-oss:120b` | ~16â€¯GB VRAM + ~64â€¯GB RAM | MOE, trÃ¨s grand, plus prÃ©cis |

**Pour le codageâ€¯:**
| ModÃ¨le | VRAM / RAM min | Remarques |
| --- | --- | --- |
| `gpt-oss:20b` | ~6â€¯GB VRAM + ~12â€¯GB RAM | MOE, rapide, performant |
| `qwen3-coder:30b-a3b` | ~8â€¯GB VRAM + ~16â€¯GB RAM | MOE, rapide, performant |
| `gemma3:27b-it-qat` | ~12â€¯GB VRAM + ~16â€¯GB RAM | Dense, bon compromis, quantification optimisÃ©e (qat) |
| `qwen3-coder:30b` | ~8â€¯GB VRAM + ~16â€¯GB RAM | Dense, encore plus performant |
| `magistral:24b` | ~8â€¯GB VRAM + ~16â€¯GB RAM | Dense, trÃ¨s performant |
| `gpt-oss:120b` | ~16â€¯GB VRAM + ~64â€¯GB RAM | MOE, trÃ¨s grand, plus prÃ©cis |

_Note pour les dÃ©butantsâ€¯:_ les LLM MOE (Mixture-Of-Experts) sont plus rapides et moins gourmands en ressources que les LLM denses.

---

<h2 id="keyboard-shortcuts">âŒ¨ï¸ Raccourcis Clavier</h2>

| Raccourci                  | Contexte                                                                                                                                                                                                                   |
| -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Ctrl + Molette de souris` | Zoom in/out de la police (dans les bulles de chat)                                                                                                                                                                         |
| `PageUp/PageDown`          | Navigation paginÃ©e (dans les bulles de chat et autres zones de texte)                                                                                                                                                      |
| `CTRL + F`                 | Recherche du `mot` sÃ©lectionnÃ© dans le chat (et dans le dialogue de validation prÃ©-inference),<br>surbrillance des rÃ©sultats avec prÃ©cÃ©dent/suivant & surbrillance des rÃ©sultats (dans les messages d'une session ouverte) |
| `CTRL + Enter`             | Envoyer le message (lorsque le champ de saisie du chat a le focus)                                                                                                                                                         |
| `Escape`                   | Annuler (en cours d'Ã©dition d'un message, lors d'une recherche)                                                                                                                                                            |
| `CTRL + S`                 | Confirmer & sauvegarder (en cours d'Ã©dition d'un message)                                                                                                                                                                  |
| `Enter`                    | Parcourir l'arborescence de fichiers <br>(lorsque la boÃ®te de chemin `Root folder` a le focus)                                                                                                                             |
| `â†‘ / â†“`                    | Naviguer parmi les chemins rÃ©cents (dans la boÃ®te de chemin `Root folder`)                                                                                                                                                 |

---

<h2 id="file-structure">ğŸ—‚ï¸ Arborescence du Projet</h2>

```
project_root/
â”œâ”€â”€ main.py                         # Point d'entrÃ©e de l'application
â”œâ”€â”€ requirements.txt                # DÃ©pendances Python
â”œâ”€â”€ README.md
â”œâ”€â”€ config.yaml                     # Configuration Qdrant
â”‚
â”œâ”€â”€ core/                               # Logique back-end (LangChain, DB, parsing,...)
â”‚   â”œâ”€â”€ config_manager.py           # Gestionnaire de configurations RÃ´le/Prompt
â”‚   â”œâ”€â”€ context_parser_config.json  # Configs multi-mode pour la gestion du contexte
â”‚   â”œâ”€â”€ context_parser.py           # Logique de gestion du contexte
â”‚   â”œâ”€â”€ database.py                 # Session DB SQLAlchemy
â”‚   â”œâ”€â”€ llm_manager.py              # Gestionnaire de LLM
â”‚   â”œâ”€â”€ llm_properties.py           # Gestionnaire des propriÃ©tÃ©s par dÃ©faut du LLM
â”‚   â”œâ”€â”€ models.py                   # ModÃ¨les ORM (Session, Message, Folder, llm_properties, prompt_config)
â”‚   â”œâ”€â”€ prompt_config_defaults.json # Configs par dÃ©faut RÃ´le/Prompt
â”‚   â”œâ”€â”€ prompt_config_manager.py    # Gestionnaire de configurations RÃ´le/Prompt
â”‚   â”œâ”€â”€ prompt_manager.py           # Construit les prompts LLM Ã  partir des configs
â”‚   â”œâ”€â”€ session_manager.py          # Gestion des sessions (et leurs messages)
â”‚   â”œâ”€â”€ message_manager/                  # Module de gestion des messages
â”‚       â”œâ”€â”€ msg_proc.py             # Gestionnaire de traitement des messages
â”‚       â”œâ”€â”€ msg_proc_utils.py       # Utilitaires de traitement des messages
â”‚   â”œâ”€â”€ rag/                              # Module de gestion du rag
â”‚       â”œâ”€â”€ handler.py              # Gestionnaire global du rag
â”‚       â”œâ”€â”€ file_loader.py          # Extracteurs de texte pour les formats supportÃ©s
â”‚       â”œâ”€â”€ indexer.py              # Indexation et rÃ©cupÃ©ration de chunks Qdrant
â”‚       â”œâ”€â”€ config.py               # Configuration du pipeline rag
â”‚   â”œâ”€â”€ theme/                            # Module de thÃ©matisation
â”‚       â”œâ”€â”€ theme_manager.py        # Gestionnaire de thÃ©matisation QSS dynamique
â”‚       â”œâ”€â”€ color_palettes.py       # Palettes de couleurs pour injection QSS
â”‚       â”œâ”€â”€ theme.qss               # Feuille de style QSS
â”‚   â”œâ”€â”€ tiktoken/                         # Module tiktoken
â”‚       â”œâ”€â”€ 9b5ad...                # modÃ¨le tiktoken local pour comptage de tokens
â”‚
â”œâ”€â”€ gui/                                # Composants GUI PyQt6
â”‚   â”œâ”€â”€ chat_panel.py               # Interface de chat LLM avec streaming
â”‚   â”œâ”€â”€ config_panel.py             # Panneau de gestion des paramÃ¨tres LLM
â”‚   â”œâ”€â”€ context_parser_panel.py     # Panneau pour sÃ©lection de mode de contexte & sÃ©lection de fichiers
â”‚   â”œâ”€â”€ gui_config.json             # ParamÃ¨tres persistants de l'UI
â”‚   â”œâ”€â”€ gui.py                      # FenÃªtre principale MainWindow
â”‚   â”œâ”€â”€ llm_worker.py               # Thread pour le streaming LLM
â”‚   â”œâ”€â”€ render_worker.py            # Thread du parseur Markdown
â”‚   â”œâ”€â”€ renderer.py                 # Parseur Markdown avec coloration syntaxique
â”‚   â”œâ”€â”€ session_panel.py            # Arbre dossier/session avec drag & drop, vue et gestion des sessions
â”‚   â”œâ”€â”€ thread_manager.py           # Gestionnaire de threads (QThread et threading.Thread...)
â”‚   â”œâ”€â”€ toolbar.py                  # Panneau de barre d'outils
â”‚   â”œâ”€â”€ widgets/                          # Petit module de widgets
â”‚       â”œâ”€â”€ context_config_dialog.py# Dialogue de configuration de contexte
â”‚       â”œâ”€â”€ prompt_validation_dialog.py # Dialogue de validation de votre requÃªte, editable avant envoi pour inference
â”‚       â”œâ”€â”€ small_widget.py         # Petits widgets
â”‚       â”œâ”€â”€ spinner.py              # Spinner Â«â€¯processing...â€¯Â»
â”‚       â”œâ”€â”€ status_indicator.py     # Indicateur vert/rouge LLM chargÃ©/dÃ©chargÃ©
â”‚       â”œâ”€â”€ search_dialog.py        # BoÃ®te de recherche CTRL+F, surbrillance des rÃ©sultats dans les bulles avec navigation prev/next
â”‚
â”œâ”€â”€ utils/                              # Modules utilitaires
â”‚   â”œâ”€â”€ qdrant_launcher.py          # Gestion du lancement/arrÃªt du binaire Qdrant
â”‚   â”œâ”€â”€ env_tools.py                # EntrÃ©e CLI utilisateur pour placer le chemin Qdrant.exe dans .env si besoin
â”‚   â”œâ”€â”€ config.yaml                 # Config Qdrant
â”‚
```

---

## ğŸ”® Perspectives Futures

-   trouver un emploiâ€¯! <--- IMPORTANT aprÃ¨s presque 5 mois sur cette application !!!!
-   Panneau de chatâ€¯: Â«â€¯continueâ€¯/â€¯regenerateâ€¯Â» des rÃ©ponses LLM
-   Sauvegarder les messages rendus en HTML pour Ã©viter le rendu Ã  la volÃ©e
-   Abstraction de la gestion des serveurs LLM afin d'intÃ©grer llamacpp et/ou LMStudio comme fournisseur LLM
-   Autoriser l'usage de l'API OpenAI pour des requÃªtes LLM distantes
-   PossibilitÃ© d'imbriquer des dossiers de session dans d'autres dossiers de session
-   CrÃ©ation et orchestration d'agents LangChain
-   RÃ©sumÃ© structurÃ© multi-fichiers basÃ© sur le RAG
-   Migration de la base SQLite vers PostgreSQL(...?)
-   IntÃ©gration de la gestion d'images pour les LLM capables de vision
-   Gestion multilingue de l'interfaceâ€¯: traductions... (pour les rÃ´les/prompt par dÃ©faut, c'est dÃ©jÃ  fait!)
-   Collaborationsâ€¯?

---

<h2 id="license">ğŸ“œ Licence</h2>

Ce projet est distribuÃ© sous licence GPLâ€¯v3. Voir le fichier [LICENSE](https://github.com/python-qt-tools/PyQt6-stubs/blob/main/LICENSE) pour plus de dÃ©tails.

### Licences Tierces

-   [PyQt6](https://github.com/python-qt-tools/PyQt6-stubs/blob/main/LICENSE) - GPLâ€¯v3
-   [LangChain](https://github.com/langchain-ai/langchain/blob/master/LICENSE) - MIT
-   [Qdrant](https://github.com/qdrant/qdrant/blob/master/LICENSE) - Apacheâ€¯2.0
-   [Ollama](https://github.com/ollama/ollama/blob/main/LICENSE) - MIT
